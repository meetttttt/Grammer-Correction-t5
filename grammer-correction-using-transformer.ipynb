{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [Fine-Tune a Transformer Model for Grammar Correction](https://www.vennify.ai/fine-tune-grammar-correction/)\n|-> Main objective is to correct grammer of text (only english language).\n\n|-> Train T-5 Model from scratch for the task of grammer correction.\n\n|-> Save the model and do Inference.\n\n|-> Further Improvement.","metadata":{}},{"cell_type":"markdown","source":"# Example:","metadata":{}},{"cell_type":"markdown","source":"![](https://production-media.paperswithcode.com/tasks/gec_foTfIZW.png)","metadata":{}},{"cell_type":"markdown","source":"# Table of content:\n- Introduction\n- Installation\n- Data Collection\n- Data Examination\n- Dataset Preprocessing\n- Before Training Evaluating\n- Training\n- After Training Evaluating\n- Inference","metadata":{}},{"cell_type":"markdown","source":"# Introduction:\n- In linguistics, the grammar of a natural language is its set of structural constraints on speakers' or writers' composition of clauses, phrases, and words.\n- A grammar checker, in computing terms, is a program, or part of a program, that attempts to verify written text for grammatical correctness.\n- Here in Grammer Correction we will be using [T5 Model](https://huggingface.co/docs/transformers/model_doc/t5) (only for English Language).\n- T5 was created by Google AI and released to the world for anyone to download and use.\n- T5 is an encoder-decoder model and converts all NLP problems into a text-to-text format. It is trained using teacher forcing. This means that for training, we always need an input sequence and a corresponding target sequence.\n- We'll use Python package called [Happy Transformer](https://happytransformer.com/). \n- Happy Transformer is built on top of Hugging Face's Transformers library and makes it easy to implement and train transformer models with just a few lines of code. ","metadata":{}},{"cell_type":"markdown","source":"# Installation: \n- We need to install happytransformer using following command.\n- pip install happytransformer.\n- Read more about [pypi](https://pypi.org/project/happytransformer/)\n- [Documentation](https://happytransformer.com/)","metadata":{}},{"cell_type":"code","source":"\"\"\" Installation of library are mentioned here \"\"\"\n!pip install happytransformer \nfrom IPython.display import clear_output\nclear_output()","metadata":{"execution":{"iopub.status.busy":"2023-01-01T04:50:02.037255Z","iopub.execute_input":"2023-01-01T04:50:02.037598Z","iopub.status.idle":"2023-01-01T04:50:15.047808Z","shell.execute_reply.started":"2023-01-01T04:50:02.037492Z","shell.execute_reply":"2023-01-01T04:50:15.046633Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\"\"\" Imports are mentioned here \"\"\"\n\nimport csv\nfrom datasets import load_dataset\nfrom happytransformer import TTSettings\nfrom happytransformer import TTTrainArgs\nfrom happytransformer import HappyTextToText","metadata":{"execution":{"iopub.status.busy":"2023-01-01T04:50:15.050281Z","iopub.execute_input":"2023-01-01T04:50:15.050773Z","iopub.status.idle":"2023-01-01T04:50:25.655125Z","shell.execute_reply.started":"2023-01-01T04:50:15.050731Z","shell.execute_reply":"2023-01-01T04:50:25.653978Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Model\n- T5 comes in several different sizes, and we'll use the base model, which has 220 million parameters.\n- T5 is a text-to-text model, meaning given text, it generated a standalone piece of text based on the input. \n- Thus, we'll import a class called HappyTextToText from Happy Transformer, which we'll use to load the model.\n- We'll provide the model type (T5) to the first position parameter and the model name (t5-base) to the second.\n- If you want to read more about T5 you can find the resouces below.\n","metadata":{}},{"cell_type":"code","source":"\"\"\" Model \"\"\"\n\nhappy_tt = HappyTextToText(\"T5\", \"t5-base\")","metadata":{"execution":{"iopub.status.busy":"2023-01-01T04:50:25.657664Z","iopub.execute_input":"2023-01-01T04:50:25.659304Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ec1d37455af4eda80a43a9682592393"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/850M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af820c0a0ffa4f81bd3fa4153f3b5ed3"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Data Collection\n- The [dataset](https://huggingface.co/datasets/jfleg) is available on Hugging Face's datasets distribution network and can be accessed using their Datasets library. \n- Since this library is a dependency for Happy Transformer, we do not need to install it and can go straight to importing a function called load_dataset from the library.  ","metadata":{}},{"cell_type":"code","source":"train_dataset = load_dataset(\"jfleg\", split='validation[:]')\n\neval_dataset = load_dataset(\"jfleg\", split='test[:]')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Examination  \n- We just successfully downloaded the dataset.\n- Let's now explore it by iterating over some cases. Both the train and eval datasets are structured the same way and have two features, sentences and corrections. \n- The sentence feature contains a single string for each case, while the correction feature contains a list of 4 human-generated corrections.","metadata":{}},{"cell_type":"code","source":"for case in train_dataset[\"corrections\"][:2]:\n    print(case)\n    print(case[0])\n    print(\"--------------------------------------------------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing  \n- Now, we must process the into the proper format for Happy Transformer. \n- We need to structure both of the training and evaluating data into the same format, which is a CSV file with two columns: input and target.\n- The input column contains grammatically incorrect text, and the target column contains text that is the corrected version of the text from the target column.","metadata":{"execution":{"iopub.status.busy":"2022-12-31T05:01:27.641636Z","iopub.status.idle":"2022-12-31T05:01:27.642392Z","shell.execute_reply.started":"2022-12-31T05:01:27.642145Z","shell.execute_reply":"2022-12-31T05:01:27.642168Z"}}},{"cell_type":"code","source":"def generate_csv(csv_path, dataset):\n    with open(csv_path, 'w', newline='') as csvfile:\n        writter = csv.writer(csvfile)\n        writter.writerow([\"input\", \"target\"])\n        for case in dataset:\n     \t    # Adding the task's prefix to input \n            input_text = \"grammar: \" + case[\"sentence\"]\n            for correction in case[\"corrections\"]:\n                # a few of the cases contain blank strings. \n                if input_text and correction:\n                    writter.writerow([input_text, correction])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_csv(\"train.csv\", train_dataset)\ngenerate_csv(\"eval.csv\", eval_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Before Training Evaluating\n- We'll evaluate the model before and after fine-tuning using a common metric called loss. \n- Loss can be described as how \"wrong\" the model's predictions are compared to the correct answers. \n- So, if the loss decreases after fine-tuning, then that suggests the model learned.\n- It's important that we use separate data for training and evaluating to show that the model can generalize its obtained knowledge to solve unseen cases.","metadata":{}},{"cell_type":"code","source":"before_result = happy_tt.eval(\"eval.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The result is a dataclass object with a single variable called loss, which we can isolate as shown below.","metadata":{}},{"cell_type":"code","source":"print(\"Before loss:\", before_result.loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training\n- Let's now train the model. \n- We can do so by calling happy_tt's train() method. \n- For simplicity, we'll use the default parameters other than the batch size which we'll increase to 8.\n- If you experience an out of memory error,  then I suggest you reduce the batch size. \n- You can visit this [webpage](https://happytransformer.com/text-to-text/finetuning/) to learn how to modify various parameters like the learning rate and the number of epochs.","metadata":{}},{"cell_type":"code","source":"args = TTTrainArgs(batch_size=8)\nhappy_tt.train(\"train.csv\", args=args)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# After Training Evaluating\n- Like before, let's determine the model's loss.","metadata":{}},{"cell_type":"code","source":"before_loss = happy_tt.eval(\"eval.csv\")\n\nprint(\"After loss: \", before_loss.loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference\n- Let's now use the model to correct the grammar of examples we'll provide it.\n- To accomplish this, we'll use happy_tt's generate_text() method. \n- We'll also use an algorithm called beam search for the generation. \n- You can view the different text generation parameters you can modify on this [webpage](https://happytransformer.com/text-to-text/settings/), along with different configurations you could use for common algorithms.","metadata":{}},{"cell_type":"code","source":"beam_settings =  TTSettings(num_beams=5, min_length=1, max_length=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Example1: \"\"\"\nexample_1 = \"grammar: This sentences, has bads grammar and spelling!\"\nresult_1 = happy_tt.generate_text(example_1, args=beam_settings)\nprint(result_1.text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Example2: \"\"\"\n\nexample_2 = \"grammar: I am enjoys, writtings articles ons AI and I also enjoyed write articling on AI.\"\n\nresult_2 = happy_tt.generate_text(example_2, args=beam_settings)\nprint(result_2.text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Further Improvement:\n- I suggest transferring some of the evaluating cases to the training data and then optimize the hyperparameters by applying a technique like grid search. \n- You can then include the evaluating cases in the training set to fine-tune a final model using your best set of hyperparameters.\n- Even we can try multiple languages to support multilinguality.\n- Add custom layers to refine output.\n- Try other models as well.","metadata":{}},{"cell_type":"markdown","source":"# Additional Resources:\n- [Transformers](https://towardsdatascience.com/transformers-89034557de14)\n- [T5](https://paperswithcode.com/method/t5)\n- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)\n- [Hugging Face](https://huggingface.co/)","metadata":{}},{"cell_type":"markdown","source":"# The End","metadata":{}}]}